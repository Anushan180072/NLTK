{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc7252fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec1eecad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f94c336",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4f8e95",
   "metadata": {},
   "source": [
    "\n",
    "Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d99b9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**word tokenization**\n",
      "\n",
      "['Hi', ',', 'How', 'are', 'you', 'doing', 'today', '?', 'You', 'got', 'a', 'nice', 'job', 'at', 'IBM', '.', 'Wow', 'thats', 'an', 'awesome', 'car', '.', 'Weather', 'is', 'great', '.']\n",
      "**sentence tokenization**\n",
      "\n",
      "['Hi , How are you doing today?', 'You got a nice job at IBM.', 'Wow thats an awesome car.', 'Weather is great.']\n"
     ]
    }
   ],
   "source": [
    "example_text='Hi , How are you doing today? You got a nice job at IBM. Wow thats an awesome car. Weather is great.'\n",
    "print(\"**word tokenization**\\n\")\n",
    "print(word_tokenize(example_text))\n",
    "print(\"**sentence tokenization**\\n\")\n",
    "print(sent_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca324ff5",
   "metadata": {},
   "source": [
    "\n",
    "Stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23cb5139",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\N.Anusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db4b2032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "253db94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'been', 'i', 'at', \"aren't\", 'here', 'shan', 'if', 'we', 'how', 'out', 'is', 'him', 'were', 'mightn', 'which', 'until', 'but', 'again', 'himself', 'he', 'with', 'below', 'and', 'between', 's', 'having', 'then', 'our', 've', 'isn', 'most', 'ours', 'ain', 'those', 'its', 'my', 'there', 'from', 'who', 'hadn', \"haven't\", 'or', 'his', 'yours', 'nor', 'hasn', 'will', 'an', 'to', 'the', \"should've\", 'each', 't', 'own', 'while', 'same', 'being', 'myself', 'doing', 'had', \"hadn't\", \"mightn't\", 'wouldn', \"you'd\", 'she', 'aren', \"that'll\", 'when', 'after', 'has', 'in', \"you're\", 'where', \"doesn't\", 'other', 'such', \"shouldn't\", 'by', 'be', 'all', 'does', \"hasn't\", \"isn't\", 'off', 'can', \"couldn't\", \"wouldn't\", 'am', 'of', 'that', 'because', 'once', 'only', 'needn', 'weren', 'did', 'into', 'further', 'yourselves', 'shouldn', \"you've\", 'd', 'her', 'against', 'y', 'your', 'now', 'ma', 'down', 'few', 'ourselves', 'have', 'whom', 'themselves', 'didn', 'so', \"needn't\", 'this', \"don't\", 'o', 'itself', 'theirs', 'm', \"you'll\", 'was', 'couldn', 'above', 'just', 'don', 'll', 'too', 'very', \"it's\", 'about', 'both', 'doesn', 'herself', 'should', 'than', 'on', 'over', 'do', 'during', 'you', 'under', 'no', 'wasn', 'yourself', \"mustn't\", \"shan't\", 'why', \"didn't\", 'what', \"wasn't\", 'mustn', 'are', 'a', 'their', \"she's\", 'before', 'they', 'some', 'hers', 'through', 'any', 'haven', \"weren't\", 'won', 'these', 'as', 'them', 'up', 'more', 'it', \"won't\", 'me', 'for', 're', 'not'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "780e06d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', ',', 'How', 'today', '?', 'You', 'got', 'nice', 'job', 'IBM', '.', 'Wow', 'thats', 'awesome', 'car', '.', 'Weather', 'great', '.']\n"
     ]
    }
   ],
   "source": [
    "example_text ='Hi , How are you doing today? You got a nice job at IBM. Wow thats an awesome car. Weather is great.'\n",
    "words = word_tokenize(example_text)\n",
    "filtered_sentence = []\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3341c726",
   "metadata": {},
   "source": [
    "\n",
    "Stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f686d121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c9c888b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi , how are you do today ?', 'you got a nice job at ibm .', 'wow that an awesom car .', 'weather is great .']\n"
     ]
    }
   ],
   "source": [
    "txt = 'Hi , How are you doing today? You got a nice job at IBM. Wow thats an awesome car. Weather is great.'\n",
    "sentences = sent_tokenize(txt)\n",
    "stemmer = PorterStemmer()\n",
    "new_sentence = []\n",
    "for i in range(len(sentences)):\n",
    "    words = word_tokenize(sentences[i])\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    new_sentence.append(' '.join(words))\n",
    "print(new_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2efe4b0",
   "metadata": {},
   "source": [
    "Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1f72026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\N.Anusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9a69ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi , How are you doing today ?', 'You got a nice job at IBM .', 'Wow thats an awesome car .', 'Weather is great .']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "txt = 'Hi , How are you doing today? You got a nice job at IBM. Wow thats an awesome car. Weather is great.'\n",
    "sentences = sent_tokenize(txt)\n",
    "lemmtizer = WordNetLemmatizer()\n",
    "new__lemmatize_sentence = []\n",
    "for i in range(len(sentences)):\n",
    "    words = word_tokenize(sentences[i])\n",
    "    words = [lemmtizer.lemmatize(word) for word in words]\n",
    "    new__lemmatize_sentence.append(' '.join(words))\n",
    "print(new__lemmatize_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c857c78b",
   "metadata": {},
   "source": [
    "\n",
    "Part of Speech Tagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b6a6ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "sample_text =\" Hi , How are you doing today? You got a nice job at IBM. Wow thats an awesome car. Weather is great.\"\n",
    "cust_tokenizer = PunktSentenceTokenizer(sample_text)\n",
    "tokenized = cust_tokenizer.tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9e8bdce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech Tagging Output\n",
      "[('Hi', 'NNP'), (',', ','), ('How', 'NNP'), ('are', 'VBP'), ('you', 'PRP'), ('doing', 'VBG'), ('today', 'NN'), ('?', '.')]\n",
      "[('You', 'PRP'), ('got', 'VBD'), ('a', 'DT'), ('nice', 'JJ'), ('job', 'NN'), ('at', 'IN'), ('IBM', 'NNP'), ('.', '.')]\n",
      "[('Wow', 'NNP'), ('thats', 'VBZ'), ('an', 'DT'), ('awesome', 'JJ'), ('car', 'NN'), ('.', '.')]\n",
      "[('Weather', 'NNP'), ('is', 'VBZ'), ('great', 'JJ'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Speech Tagging Output\")\n",
    "def process_text():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "process_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a13d9a",
   "metadata": {},
   "source": [
    "\n",
    "Chunking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4af98b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech Tagging Output\n",
      "[('Temperature', 'NN'), ('of', 'IN'), ('New', 'NNP'), ('York', 'NNP'), ('.', '.')]\n",
      "[('Here', 'RB'), ('Temperature', 'NNP'), ('is', 'VBZ'), ('the', 'DT'), ('intention', 'NN'), ('and', 'CC'), ('New', 'NNP'), ('York', 'NNP'), ('is', 'VBZ'), ('the', 'DT'), ('entity', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "sample_text =\"Temperature of New York. Here Temperature is the intention and New York is the entity\"\n",
    "cust_tokenizer = PunktSentenceTokenizer(sample_text)\n",
    "tokenized = cust_tokenizer.tokenize(sample_text)\n",
    "print(\"Speech Tagging Output\")\n",
    "def process_text():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "process_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dd525f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunked Output\n",
      "(S Temperature/NN of/IN (Chunk New/NNP York/NNP) ./.)\n",
      "(S\n",
      "  Here/RB\n",
      "  (Chunk Temperature/NNP is/VBZ)\n",
      "  the/DT\n",
      "  intention/NN\n",
      "  and/CC\n",
      "  (Chunk New/NNP York/NNP is/VBZ)\n",
      "  the/DT\n",
      "  entity/NN)\n"
     ]
    }
   ],
   "source": [
    "print(\"Chunked Output\")\n",
    "def chunked_text():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            chunkGram = r\"\"\"Chunk:{<NNP>*<VBZ>*}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            chunked.draw()\n",
    "            print(chunked)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "chunked_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70893baf",
   "metadata": {},
   "source": [
    "\n",
    "Chinking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "99ed2cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinked Output\n",
      "(S (Chunk Temperature/NN) of/IN (Chunk New/NNP York/NNP ./.))\n",
      "(S\n",
      "  (Chunk Here/RB Temperature/NNP)\n",
      "  is/VBZ\n",
      "  the/DT\n",
      "  (Chunk intention/NN and/CC New/NNP York/NNP)\n",
      "  is/VBZ\n",
      "  the/DT\n",
      "  (Chunk entity/NN))\n"
     ]
    }
   ],
   "source": [
    "print(\"Chinked Output\")\n",
    "def chinked_text():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            chunkGram = r\"\"\"Chunk: {<.*>+}\n",
    "                                    }<VB.?|IN|DT|TO>+{\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            chunked.draw()\n",
    "            print(chunked)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "chinked_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac7b021",
   "metadata": {},
   "source": [
    "\n",
    "Named Entity Recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "859f81ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\N.Anusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\N.Anusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8c86c4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entity Output\n",
      "(S Hi/NNP my/PRP$ name/NN is/VBZ (PERSON Anusha/NNP) ./.)\n",
      "(S\n",
      "  Currently/RB\n",
      "  I/PRP\n",
      "  am/VBP\n",
      "  studying/VBG\n",
      "  at/IN\n",
      "  (ORGANIZATION Rajive/NNP Gandhi/NNP University/NNP)\n",
      "  which/WDT\n",
      "  is/VBZ\n",
      "  located/VBN\n",
      "  in/IN\n",
      "  (GPE Nuzvid/NNP))\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "sample_text =\"Hi my name is Anusha. Currently I am studying at Rajive Gandhi University which is located in Nuzvid\"\n",
    "cust_tokenizer = PunktSentenceTokenizer(sample_text)\n",
    "tokenized = cust_tokenizer.tokenize(sample_text)\n",
    "print(\"Named Entity Output\")\n",
    "def process_text():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            namedEnt = nltk.ne_chunk(tagged)\n",
    "            namedEnt.draw()\n",
    "            print(namedEnt)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "process_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001398f7",
   "metadata": {},
   "source": [
    "\n",
    "Corpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9a02886a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\N.Anusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ce238c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['au\\naux\\navec\\nce\\nces\\ndans\\nde\\ndes\\ndu\\nelle\\nen\\net\\neux\\nil\\nils\\nje\\nla\\nle\\nles\\nleur\\nlui\\nma\\nmais\\nme\\nmême\\nmes\\nmoi\\nmon\\nne\\nnos\\nnotre\\nnous\\non\\nou\\npar\\npas\\npour\\nqu\\nque\\nqui\\nsa\\nse\\nses\\nson\\nsur\\nta\\nte\\ntes\\ntoi\\nton\\ntu\\nun\\nune\\nvos\\nvotre\\nvous\\nc\\nd\\nj\\nl\\nà\\nm\\nn\\ns\\nt\\ny\\nété\\nétée\\nétées\\nétés\\nétant\\nétante\\nétants\\nétantes\\nsuis\\nes\\nest\\nsommes\\nêtes\\nsont\\nserai\\nseras\\nsera\\nserons\\nserez\\nseront\\nserais\\nserait\\nserions\\nseriez\\nseraient\\nétais\\nétait\\nétions\\nétiez\\nétaient\\nfus\\nfut\\nfûmes\\nfûtes\\nfurent\\nsois\\nsoit\\nsoyons\\nsoyez\\nsoient\\nfusse\\nfusses\\nfût\\nfussions\\nfussiez\\nfussent\\nayant\\nayante\\nayantes\\nayants\\neu\\neue\\neues\\neus\\nai\\nas\\navons\\navez\\nont\\naurai\\nauras\\naura\\naurons\\naurez\\nauront\\naurais\\naurait\\naurions\\nauriez\\nauraient\\navais\\navait\\navions\\naviez\\navaient\\neut\\neûmes\\neûtes\\neurent\\naie\\naies\\nait\\nayons\\nayez\\naient\\neusse\\neusses\\neût\\neussions\\neussiez\\neussent']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sample = stopwords.raw(\"french\")\n",
    "tok = sent_tokenize(sample)\n",
    "print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "698a545a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"i\\nme\\nmy\\nmyself\\nwe\\nour\\nours\\nourselves\\nyou\\nyou're\\nyou've\\nyou'll\\nyou'd\\nyour\\nyours\\nyourself\\nyourselves\\nhe\\nhim\\nhis\\nhimself\\nshe\\nshe's\\nher\\nhers\\nherself\\nit\\nit's\\nits\\nitself\\nthey\\nthem\\ntheir\\ntheirs\\nthemselves\\nwhat\\nwhich\\nwho\\nwhom\\nthis\\nthat\\nthat'll\\nthese\\nthose\\nam\\nis\\nare\\nwas\\nwere\\nbe\\nbeen\\nbeing\\nhave\\nhas\\nhad\\nhaving\\ndo\\ndoes\\ndid\\ndoing\\na\\nan\\nthe\\nand\\nbut\\nif\\nor\\nbecause\\nas\\nuntil\\nwhile\\nof\\nat\\nby\\nfor\\nwith\\nabout\\nagainst\\nbetween\\ninto\\nthrough\\nduring\\nbefore\\nafter\\nabove\\nbelow\\nto\\nfrom\\nup\\ndown\\nin\\nout\\non\\noff\\nover\\nunder\\nagain\\nfurther\\nthen\\nonce\\nhere\\nthere\\nwhen\\nwhere\\nwhy\\nhow\\nall\\nany\\nboth\\neach\\nfew\\nmore\\nmost\\nother\\nsome\\nsuch\\nno\\nnor\\nnot\\nonly\\nown\\nsame\\nso\\nthan\\ntoo\\nvery\\ns\\nt\\ncan\\nwill\\njust\\ndon\\ndon't\\nshould\\nshould've\\nnow\\nd\\nll\\nm\\no\\nre\\nve\\ny\\nain\\naren\\naren't\\ncouldn\\ncouldn't\\ndidn\\ndidn't\\ndoesn\\ndoesn't\\nhadn\\nhadn't\\nhasn\\nhasn't\\nhaven\\nhaven't\\nisn\\nisn't\\nma\\nmightn\\nmightn't\\nmustn\\nmustn't\\nneedn\\nneedn't\\nshan\\nshan't\\nshouldn\\nshouldn't\\nwasn\\nwasn't\\nweren\\nweren't\\nwon\\nwon't\\nwouldn\\nwouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "sample = stopwords.raw(\"english\")\n",
    "tok = sent_tokenize(sample)\n",
    "print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "07e725cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PM denies knowledge of AWB kickbacks\\nThe Prime Minister has denied he knew AWB was paying kickbacks to Iraq despite writing to the wheat exporter asking to be kept fully informed on Iraq wheat sales.', 'Letters from John Howard and Deputy Prime Minister Mark Vaile to AWB have been released by the Cole inquiry into the oil for food program.', 'In one of the letters Mr Howard asks AWB managing director Andrew Lindberg to remain in close contact with the Government on Iraq wheat sales.', \"The Opposition's Gavan O'Connor says the letter was sent in 2002, the same time AWB was paying kickbacks to Iraq though a Jordanian trucking company.\", 'He says the Government can longer wipe its hands of the illicit payments, which totalled $290 million.', '\"The responsibility for this must lay may squarely at the feet of Coalition ministers in trade, agriculture and the Prime Minister,\" he said.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import abc\n",
    "sample = abc.raw(\"rural.txt\")\n",
    "tok = sent_tokenize(sample)\n",
    "print(tok[0:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89f879aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Python310\\lib\\site-packages\\nltk\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "#print(nltk.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62748958",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
